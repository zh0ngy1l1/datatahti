{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62f7c423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import datetime\n",
    "import re\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# whitelist for topics we know are relevant.\n",
    "whitelist = [\"Tietotekniikka\", \"Internet\", \"Tv ja radio\", \"Kodintekniikka\"]\n",
    "\n",
    "\n",
    "# edit this to edit where the result files are stored\n",
    "storageLocation = \"./crawlerResults/\"\n",
    "\n",
    "pathlib.Path(storageLocation).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02f10691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getThreads(keyword, pageNumber): # returns the url for every thread from given keyword and pagenumber\n",
    "    interestingThreads = []\n",
    "\n",
    "    currentUrl = \"https://keskustelu.suomi24.fi/haku?keyword={kw}&page={pn}\".format(\n",
    "        kw=keyword, pn=pageNumber)  # make the url we're looking for\n",
    "\n",
    "    print(f\"current URL: {currentUrl}\")\n",
    "\n",
    "    response = requests.get(currentUrl)  # we get our html\n",
    "\n",
    "    # we convert our html into soup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # find all links that look like this\n",
    "    for link in soup.find_all(href=re.compile(\"https://keskustelu.suomi24.fi/t/\")):\n",
    "        interestingThreads.append(link.get('href'))  # keep their href's\n",
    "\n",
    "    print(f\"page {pageNumber} has {len(interestingThreads)} threads regarding {keyword}: {interestingThreads}\")\n",
    "    return interestingThreads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4441cdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllThreads(keyword, pageFraction): # returns a list of urls to filter and crawl. Automate getThreads, we take a certain fraction of all the pages from search with keyword. if pageFraction >=1, returns pageFraction urls instead.\n",
    "    urlList = []\n",
    "    \n",
    "    if pageFraction < 1:\n",
    "        baseUrl = \"https://keskustelu.suomi24.fi/haku?keyword=\" + keyword # the first page that you get when you search with keyword\n",
    "        \n",
    "        response = requests.get(baseUrl)  # we get our html\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser') # we convert our html into soup\n",
    "\n",
    "        pages = int(soup.find(attrs={\"class\": \"pagination-page-count\"}).string) # crazy-ass one-liner. finds the tag that shows how many pages of search results there are.\n",
    "\n",
    "        pagesToCrawl = int(pages * pageFraction) # so it doesn't take 5 years to complete. safe pageFraction is around 1E-2\n",
    "\n",
    "    else:\n",
    "        pagesToCrawl = pageFraction\n",
    "\n",
    "    print(f\"\\nbegin gathering thread url's from {pagesToCrawl} pages of keyword {keyword}.\")\n",
    "    for pageNumber in range(pagesToCrawl):\n",
    "        urlList.extend(getThreads(keyword, pageNumber + 1))\n",
    "\n",
    "    print(f\"Succesfully gathered {len(urlList)} url's from the {pagesToCrawl} pages of keyword {keyword}.\")\n",
    "\n",
    "    return urlList\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "459462bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getContent(threadUrl): # returns a JSON with all the information we're interested in, from the given URL.\n",
    "\n",
    "    response = requests.get(threadUrl)\n",
    "\n",
    "    if response.status_code >= 400:\n",
    "        raise Exception(\"dead page\")\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # the discussion but in soup tag form we don't want that\n",
    "    discussionTag = soup.find_all(type=\"application/ld+json\")[0]\n",
    "\n",
    "    # now it's json\n",
    "    discussionJson = json.loads(discussionTag.contents[0])\n",
    "    #print(json.dumps(discussionJson, indent = 4, sort_keys = True))\n",
    "\n",
    "    return discussionJson\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ddf0ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBreadcrumbs(discussionJson): # returns the list of breadcrumbs from given json of the discussion\n",
    "    breadcrumbsJson = discussionJson[2]  # the third element has the crumbs\n",
    "    breadcrumbsList = []\n",
    "    itemList = breadcrumbsJson['itemListElement']\n",
    "    for itemdata in itemList:\n",
    "        item = itemdata['item']\n",
    "        # very hard-to-navigate and counterintuitive structure.\n",
    "        breadcrumbsList.append(item['name'])\n",
    "\n",
    "    return breadcrumbsList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72e92713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crumbsRelevance(breadcrumbsList): # determine if the crumbs correspond to a relevant topic, going through relevantTopics\n",
    "    for topic in whitelist:\n",
    "        if topic in breadcrumbsList:\n",
    "            return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "131d06fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def countComments(threadContent): # returns the count of comments from given json of the post and comments\n",
    "    commentCount = len(threadContent[\"comment\"])\n",
    "    for currentComment in threadContent[\"comment\"]:\n",
    "        currentReplies = currentComment.get(\"comment\")\n",
    "        if currentReplies != None:\n",
    "            commentCount = commentCount+len(currentReplies)\n",
    "    return commentCount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "456bba46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawlUrlList(urlList, keyword=\"unspecified\", noFilter=False): # return a DataFrame with all the posts and their interesting data from given list of URLs\n",
    "\n",
    "    posts = {\"author\": [], \"category\": [], \"url\": [], \"time\": [], \"headline\": [], \"text\": [], \"commentCount\": [], \"comments\": [], \"keyword\": []}\n",
    "    \n",
    "\n",
    "    totalLength = len(urlList)  # used in progress tracking\n",
    "    doing = 0\n",
    "\n",
    "    for threadUrl in urlList:\n",
    "\n",
    "        doing = doing + 1\n",
    "        print(f\"progress: {doing}/{totalLength}\")  # track progress\n",
    "\n",
    "        # extract useful stuff\n",
    "        try:\n",
    "            # only parts of getContent returned JSON is useful. We get that.\n",
    "            allContent = getContent(threadUrl)\n",
    "        except Exception as ex:\n",
    "            print(f\"met exception: {threadUrl} is {ex}\")\n",
    "            continue\n",
    "\n",
    "        # get category\n",
    "        threadCategory = getBreadcrumbs(allContent)\n",
    "\n",
    "        # continue only if the category is relevant.\n",
    "        if crumbsRelevance(threadCategory) or noFilter: \n",
    "            \n",
    "            threadContent = allContent[3] # aside from category, all the data we want is in the 4th item\n",
    "\n",
    "            creationTime = datetime.datetime.fromisoformat(\n",
    "                threadContent[\"dateCreated\"][:-1]) # the time is a string. The string has a Z in the end indicating standart time. we don't need that\n",
    "            \n",
    "            timePast = datetime.datetime.utcnow() - creationTime\n",
    "            expiringTime = datetime.timedelta(days=3*365) # edit this to filter older/newer posts\n",
    "\n",
    "            # continue only if the post isn't too old.\n",
    "            if timePast < expiringTime:\n",
    "\n",
    "                # store all the stuff we wish to keep about the post.\n",
    "                posts[\"category\"].append(threadCategory)\n",
    "                posts[\"commentCount\"].append(countComments(threadContent))\n",
    "                posts[\"comments\"].append(threadContent[\"comment\"])\n",
    "                posts[\"headline\"].append(threadContent[\"headline\"])\n",
    "                posts[\"text\"].append(threadContent[\"text\"])\n",
    "                posts[\"time\"].append(threadContent[\"dateCreated\"])\n",
    "                posts[\"url\"].append(threadUrl)\n",
    "                posts[\"author\"].append(threadContent[\"author\"][\"name\"])\n",
    "                posts[\"keyword\"] = keyword\n",
    "\n",
    "            else:\n",
    "                print(f\"that is {timePast.days} days ago, way too old.\")\n",
    "\n",
    "        else:\n",
    "            print(f\"{threadUrl} is out-of-context.\\n\")\n",
    "\n",
    "    posts_df = pd.DataFrame(posts)\n",
    "    posts_df.set_index('url', inplace=True)\n",
    "\n",
    "    print(f\"succesfully loaded {len(posts['url'])} out of {totalLength} threads.\")\n",
    "    return posts_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3547b153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractComments(threadsDf): # return a DataFrame with all the comments and their interesting data from given DataFrame of threads\n",
    "    comments = {\"url\": [], \"time\": [], \"text\": [], \"replyCount\": [], \"parent\": []}\n",
    "\n",
    "\n",
    "    threadsDf = threadsDf[threadsDf[\"commentCount\"] != 0 ]\n",
    "\n",
    "    threadsDict = dict(threadsDf[\"comments\"])\n",
    "\n",
    "\n",
    "    for threadUrl in threadsDict.keys():\n",
    "\n",
    "        nrtext = \"\" # text for comments with no replies\n",
    "        \n",
    "        for comment in threadsDict[threadUrl]:\n",
    "            replies = comment.get(\"comment\")\n",
    "\n",
    "            if replies != None:\n",
    "\n",
    "                #add this comment into comments\n",
    "                \n",
    "                comments[\"url\"].append(comment[\"url\"])\n",
    "                comments[\"time\"].append(comment[\"dateCreated\"])\n",
    "                comments[\"text\"].append(comment[\"text\"])\n",
    "                comments[\"replyCount\"].append(len(replies))\n",
    "                comments[\"parent\"].append(threadUrl)\n",
    "\n",
    "\n",
    "                #add its replies into comments\n",
    "                rtext = \"\"\n",
    "\n",
    "                for reply in replies:\n",
    "                    rtext = rtext + reply[\"text\"] + \" \" # separator\n",
    "                \n",
    "                rTime = replies[-1][\"dateCreated\"] # newest reply\n",
    "                rUrl = replies[-1][\"url\"] # newest reply\n",
    "\n",
    "                comments[\"url\"].append(rUrl)\n",
    "                comments[\"time\"].append(rTime)\n",
    "                comments[\"text\"].append(rtext)\n",
    "                comments[\"replyCount\"].append(0) # zero for all except comments with replies\n",
    "                comments[\"parent\"].append(threadUrl)\n",
    "            \n",
    "            else:\n",
    "                nrtext = nrtext + comment[\"text\"] + \" \" # separator\n",
    "                nrTime = comment[\"dateCreated\"] # newest no-reply comment\n",
    "                nrUrl = comment[\"url\"] # newest no-reply comment\n",
    "        \n",
    "        comments[\"url\"].append(nrUrl)\n",
    "        comments[\"time\"].append(nrTime)\n",
    "        comments[\"text\"].append(nrtext)\n",
    "        comments[\"replyCount\"].append(0) # zero \n",
    "        comments[\"parent\"].append(threadUrl)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    comments_df = pd.DataFrame(comments)\n",
    "    comments_df.set_index('url', inplace=True)\n",
    "\n",
    "    print(f\"succesfully extracted {len(comments_df.index)} rows of data from {len(threadsDf.index)} threads.\")\n",
    "\n",
    "    return comments_df\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "936ef0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categoryCounter(posts_df): # returns a sorted series with count for every maincategory, from given posts DataFrame\n",
    "\n",
    "    # get all categories\n",
    "    allCategories = posts_df[\"category\"].tolist()\n",
    "\n",
    "    # use the third element to determine if relevant.\n",
    "    mainCategories = [category[2] for category in allCategories if len(category)>2]\n",
    "\n",
    "    # if not possible, then use the second element\n",
    "    mainCategories.extend([category[1] for category in allCategories if len(category)==2])\n",
    "\n",
    "    mainCounts = Counter(mainCategories)  # Count the occurrences of each element\n",
    "\n",
    "    # convert to series\n",
    "    categorySeries = pd.Series(list(mainCounts.values()), index=list(\n",
    "        mainCounts.keys()))\n",
    "\n",
    "    # sort by count\n",
    "    categorySeries.sort_values(inplace=True, ascending=False)\n",
    "\n",
    "    return categorySeries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b72593ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findSharedCategories(dfDict): #returns a series of shared category name and total posts with said category, from given dict of {\"name\": DataFrame}\n",
    "\n",
    "    cSeriesList = [categoryCounter(df) for df in dfDict.values()] # a list of series returned by categoryCounter from elements in allDataFrames\n",
    "\n",
    "    topNToKeep = 10 #change this to change top howmany do we keep from each category.\n",
    "\n",
    "    sharedCategorySeries = cSeriesList[0].head(topNToKeep).copy(deep=False) # copies the first series's top n rows, in order to do addition pd.Series properly\n",
    "\n",
    "    #print(sharedCategorySeries)\n",
    "\n",
    "    for categorySeries in cSeriesList[1:]: # first one already accounted for\n",
    "        seriesTopCategories = categorySeries.head(topNToKeep) \n",
    "\n",
    "        #print(seriesTopCategories)\n",
    "\n",
    "        sharedCategorySeries = sharedCategorySeries + seriesTopCategories # only the categories shared by all 3 are kept. rest have value NaN\n",
    "        \n",
    "        # sharedCategorySeries = sharedCategorySeries.add(seriesTopCategories, fill_value=-10)\n",
    "\n",
    "    sharedCategorySeries = sharedCategorySeries.dropna().sort_values(ascending=False) # dropna() gets rid of the ones with NaN\n",
    "\n",
    "    return sharedCategorySeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2541ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def storeDataFrames(dfDict): # store the given dictionary {\"name\": DataFrame} as separate csvs, return a dict with {\"name\": filename for csv}\n",
    "\n",
    "    csvDict = {}\n",
    "\n",
    "    for dfName in dfDict.keys(): # for every df in the inputted dict:\n",
    "        \n",
    "        csvName = \"{loc}{name}_data_{date}.csv\".format(loc=storageLocation, name=dfName, date=(datetime.date.today()).strftime('%Y-%m')) # create the name of the file\n",
    "\n",
    "        dfDict[dfName].to_csv(csvName) # save df as csv\n",
    "\n",
    "        csvDict[dfName] = csvName # add to the dictionary that this function returns\n",
    "    \n",
    "    return csvDict\n",
    "\n",
    "def readDataFrames(csvDict): # reverse for above function\n",
    "\n",
    "    dfDict = {}\n",
    "\n",
    "    for dfName in csvDict.keys(): # for every df in the inputted dict:\n",
    "        \n",
    "        csvName = csvDict[dfName] # get the name of the csv file\n",
    "\n",
    "        dfDict[dfName] = pd.read_csv(csvName) # load the DataFrame from csv file and add to the dictionary that this function returns\n",
    "\n",
    "    return dfDict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7452f91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "begin gathering thread url's from 1 pages of keyword dna.\n",
      "current URL: https://keskustelu.suomi24.fi/haku?keyword=dna&page=1\n",
      "page 1 has 20 threads regarding dna: ['https://keskustelu.suomi24.fi/t/17878484/ty-prepaid-liittyma-emdnaem', 'https://keskustelu.suomi24.fi/t/17867328/emdnaem-4g-hehh', 'https://keskustelu.suomi24.fi/t/17856629/emdnaem-on-kusettaa-reilust', 'https://keskustelu.suomi24.fi/t/17852538/emdnaemn-liittyma-hairitse', 'https://keskustelu.suomi24.fi/t/17850494/kuinka-postetaan-emdnaem-oyj-nettisivuille-t', 'https://keskustelu.suomi24.fi/t/17816402/vat-teleoperaattori-emdnaem-oyjn-asiakaskokemu', 'https://keskustelu.suomi24.fi/t/17799626/emdnaem-testi---miten-tieta', 'https://keskustelu.suomi24.fi/t/17753098/minuakin-emdnaem-huijasi-', 'https://keskustelu.suomi24.fi/t/17748353/emdnaem-virus-pesake', 'https://keskustelu.suomi24.fi/t/17747680/emdnaem-oikea-virus-pesa-pu', 'https://keskustelu.suomi24.fi/t/17880563/onko-nain-emdnaem-prepaid', 'https://keskustelu.suomi24.fi/t/17876295/emdnaem-mesh-asennuksen-hin', 'https://keskustelu.suomi24.fi/t/17875991/emdnaem-vai-telian', 'https://keskustelu.suomi24.fi/t/17872235/tutkijat-muuttivat-kanan-alkion-jalkojen-suomut-hoyheniksi', 'https://keskustelu.suomi24.fi/t/17870696/lajirajojen-ylittamisen-mahdottomuus', 'https://keskustelu.suomi24.fi/t/17634573/ululaiset-lankeatte-emdnaemhan', 'https://keskustelu.suomi24.fi/t/17864338/emme-ole-aponoita', 'https://keskustelu.suomi24.fi/t/17627345/emdnaemn-tolkuton-mainosta', 'https://keskustelu.suomi24.fi/t/17863437/emdnaem-palsta-loppuu', 'https://keskustelu.suomi24.fi/t/17619902/kinainen-keskustelu-emdnaemn-olemuksesta-ja-ro']\n",
      "Succesfully gathered 20 url's from the 1 pages of keyword dna.\n",
      "progress: 1/20\n",
      "progress: 2/20\n",
      "progress: 3/20\n",
      "https://keskustelu.suomi24.fi/t/17856629/emdnaem-on-kusettaa-reilust is out-of-context.\n",
      "\n",
      "progress: 4/20\n",
      "progress: 5/20\n",
      "progress: 6/20\n",
      "progress: 7/20\n",
      "https://keskustelu.suomi24.fi/t/17799626/emdnaem-testi---miten-tieta is out-of-context.\n",
      "\n",
      "progress: 8/20\n",
      "progress: 9/20\n",
      "https://keskustelu.suomi24.fi/t/17748353/emdnaem-virus-pesake is out-of-context.\n",
      "\n",
      "progress: 10/20\n",
      "https://keskustelu.suomi24.fi/t/17747680/emdnaem-oikea-virus-pesa-pu is out-of-context.\n",
      "\n",
      "progress: 11/20\n",
      "progress: 12/20\n",
      "progress: 13/20\n",
      "progress: 14/20\n",
      "https://keskustelu.suomi24.fi/t/17872235/tutkijat-muuttivat-kanan-alkion-jalkojen-suomut-hoyheniksi is out-of-context.\n",
      "\n",
      "progress: 15/20\n",
      "https://keskustelu.suomi24.fi/t/17870696/lajirajojen-ylittamisen-mahdottomuus is out-of-context.\n",
      "\n",
      "progress: 16/20\n",
      "progress: 17/20\n",
      "https://keskustelu.suomi24.fi/t/17864338/emme-ole-aponoita is out-of-context.\n",
      "\n",
      "progress: 18/20\n",
      "progress: 19/20\n",
      "progress: 20/20\n",
      "https://keskustelu.suomi24.fi/t/17619902/kinainen-keskustelu-emdnaemn-olemuksesta-ja-ro is out-of-context.\n",
      "\n",
      "succesfully loaded 12 out of 20 threads.\n",
      "Created new DataFrame dna with 12 threads.\n",
      "\n",
      "\n",
      "begin gathering thread url's from 1 pages of keyword elisa.\n",
      "current URL: https://keskustelu.suomi24.fi/haku?keyword=elisa&page=1\n",
      "page 1 has 20 threads regarding elisa: ['https://keskustelu.suomi24.fi/t/17882623/emelisaem-aaltola---kisajarje', 'https://keskustelu.suomi24.fi/t/17882247/taa-navetassa-myos-emelisaem-aaltola', 'https://keskustelu.suomi24.fi/t/17875002/profetoiko-emelisaem-vaarin-sodan-mooabi', 'https://keskustelu.suomi24.fi/t/17861745/emelisaem-muuttanut-laskutust', 'https://keskustelu.suomi24.fi/t/17860061/emelisaemn-5g--yhteys', 'https://keskustelu.suomi24.fi/t/17847100/emelisaembeth-naucler-on-hiuk', 'https://keskustelu.suomi24.fi/t/17812132/miten-voin-vaihtaa-emelisaemn-(netti-fi)-sahkopo', 'https://keskustelu.suomi24.fi/t/17806733/emelisaem-oberg-juoru', 'https://keskustelu.suomi24.fi/t/17802855/kaa-ihmetelko-etta-emelisaem-nostaa-liittymien-h', 'https://keskustelu.suomi24.fi/t/17772514/onko-emelisaem-liittymarasistinen-', 'https://keskustelu.suomi24.fi/t/17757437/emelisaem-adsl', 'https://keskustelu.suomi24.fi/t/17756458/emelisaem-saatovoimabisneksee', 'https://keskustelu.suomi24.fi/t/17746121/emelisaem---p-a-s-k-a', 'https://keskustelu.suomi24.fi/t/17666819/emelisaemn-myyjat', 'https://keskustelu.suomi24.fi/t/17883044/lehma-elaa---elaako-emelisaem--', 'https://keskustelu.suomi24.fi/t/17882820/emelisaem-aaltola-puheenjohta', 'https://keskustelu.suomi24.fi/t/17881676/emelisaem--aaltola--vihrea--j', 'https://keskustelu.suomi24.fi/t/17881640/raiii-kalle-r--ei-piitannut-tormaysiehman-voinnista', 'https://keskustelu.suomi24.fi/t/17880430/raamattu-ei-ole-luotettava', 'https://keskustelu.suomi24.fi/t/17874954/hirvea-rikos!-murhaa-pahempi---']\n",
      "Succesfully gathered 20 url's from the 1 pages of keyword elisa.\n",
      "progress: 1/20\n",
      "https://keskustelu.suomi24.fi/t/17882623/emelisaem-aaltola---kisajarje is out-of-context.\n",
      "\n",
      "progress: 2/20\n",
      "https://keskustelu.suomi24.fi/t/17882247/taa-navetassa-myos-emelisaem-aaltola is out-of-context.\n",
      "\n",
      "progress: 3/20\n",
      "https://keskustelu.suomi24.fi/t/17875002/profetoiko-emelisaem-vaarin-sodan-mooabi is out-of-context.\n",
      "\n",
      "progress: 4/20\n",
      "progress: 5/20\n",
      "https://keskustelu.suomi24.fi/t/17860061/emelisaemn-5g--yhteys is out-of-context.\n",
      "\n",
      "progress: 6/20\n",
      "https://keskustelu.suomi24.fi/t/17847100/emelisaembeth-naucler-on-hiuk is out-of-context.\n",
      "\n",
      "progress: 7/20\n",
      "progress: 8/20\n",
      "https://keskustelu.suomi24.fi/t/17806733/emelisaem-oberg-juoru is out-of-context.\n",
      "\n",
      "progress: 9/20\n",
      "progress: 10/20\n",
      "progress: 11/20\n",
      "progress: 12/20\n",
      "https://keskustelu.suomi24.fi/t/17756458/emelisaem-saatovoimabisneksee is out-of-context.\n",
      "\n",
      "progress: 13/20\n",
      "progress: 14/20\n",
      "progress: 15/20\n",
      "https://keskustelu.suomi24.fi/t/17883044/lehma-elaa---elaako-emelisaem-- is out-of-context.\n",
      "\n",
      "progress: 16/20\n",
      "https://keskustelu.suomi24.fi/t/17882820/emelisaem-aaltola-puheenjohta is out-of-context.\n",
      "\n",
      "progress: 17/20\n",
      "https://keskustelu.suomi24.fi/t/17881676/emelisaem--aaltola--vihrea--j is out-of-context.\n",
      "\n",
      "progress: 18/20\n",
      "https://keskustelu.suomi24.fi/t/17881640/raiii-kalle-r--ei-piitannut-tormaysiehman-voinnista is out-of-context.\n",
      "\n",
      "progress: 19/20\n",
      "https://keskustelu.suomi24.fi/t/17880430/raamattu-ei-ole-luotettava is out-of-context.\n",
      "\n",
      "progress: 20/20\n",
      "https://keskustelu.suomi24.fi/t/17874954/hirvea-rikos!-murhaa-pahempi--- is out-of-context.\n",
      "\n",
      "succesfully loaded 7 out of 20 threads.\n",
      "Created new DataFrame elisa with 7 threads.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pageFraction = 1\n",
    "#keywordList = [\"dna\"]\n",
    "keywordList = [\"dna\", \"elisa\", \"telia\", \"sonera\"][:2]\n",
    "allDataFrames = {}\n",
    "\n",
    "for keyword in keywordList:\n",
    "\n",
    "    currentThreads = getAllThreads(keyword, pageFraction)\n",
    "    currentDf = crawlUrlList(currentThreads, keyword, noFilter=False)\n",
    "    currentDf.sort_values(by=\"commentCount\", inplace=True, ascending=False) # sort by commentCount (popularity)\n",
    "    allDataFrames[keyword] = currentDf\n",
    "    print(f\"Created new DataFrame {keyword} with {len(currentDf.index)} threads.\\n\")\n",
    "    \n",
    "\n",
    "allcsvDicts = storeDataFrames(allDataFrames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a426595b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "succesfully extracted 33 rows of data from 9 threads.\n",
      "succesfully extracted 47 rows of data from 7 threads.\n"
     ]
    }
   ],
   "source": [
    "allComments = pd.DataFrame()\n",
    "for df in allDataFrames.values():\n",
    "    comments = extractComments(df)\n",
    "    allComments = pd.concat([allComments, comments])\n",
    "\n",
    "allComments.sort_values(by=\"replyCount\", inplace=True, ascending=False) # sort by commentCount (popularity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27ea9b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                        time  \\\n",
      "url                                                                            \n",
      "https://keskustelu.suomi24.fi/t/17812132/miten-...  2023-04-05T12:52:41.000Z   \n",
      "https://keskustelu.suomi24.fi/t/17746121/elisa-...  2023-02-15T13:14:13.000Z   \n",
      "https://keskustelu.suomi24.fi/t/17746121/elisa-...  2023-03-08T07:37:30.000Z   \n",
      "https://keskustelu.suomi24.fi/t/17746121/elisa-...  2023-02-17T19:08:39.000Z   \n",
      "https://keskustelu.suomi24.fi/t/17746121/elisa-...  2023-02-16T11:53:14.000Z   \n",
      "...                                                                      ...   \n",
      "https://keskustelu.suomi24.fi/t/17746121/elisa-...  2023-02-16T08:23:42.000Z   \n",
      "https://keskustelu.suomi24.fi/t/17816402/mitka-...  2023-05-05T20:42:42.000Z   \n",
      "https://keskustelu.suomi24.fi/t/17746121/elisa-...  2023-03-08T18:52:55.000Z   \n",
      "https://keskustelu.suomi24.fi/t/17746121/elisa-...  2023-02-17T07:50:16.000Z   \n",
      "https://keskustelu.suomi24.fi/t/17772514/onko-e...  2023-03-09T12:11:14.000Z   \n",
      "\n",
      "                                                                                                 text  \\\n",
      "url                                                                                                     \n",
      "https://keskustelu.suomi24.fi/t/17812132/miten-...  Gmail-viestit ovat kauppatavaraa. Kuolemasi jä...   \n",
      "https://keskustelu.suomi24.fi/t/17746121/elisa-...                         Miksi et osaa kirjoittaa ?   \n",
      "https://keskustelu.suomi24.fi/t/17746121/elisa-...  Entä jos ei tarvitse jättimäistä 1000 Mt:n tal...   \n",
      "https://keskustelu.suomi24.fi/t/17746121/elisa-...  Aloittaja puhuu asiaa.<br /><br />Alunperin sä...   \n",
      "https://keskustelu.suomi24.fi/t/17746121/elisa-...  Luuletko vajakki, että ne palvelut tuotetaan i...   \n",
      "...                                                                                               ...   \n",
      "https://keskustelu.suomi24.fi/t/17746121/elisa-...  Etkös sä juonut koko eläkkeesi! Joopa joo...<b...   \n",
      "https://keskustelu.suomi24.fi/t/17816402/mitka-...  Samma här. Asiakaspalvelija jätti kertomatta, ...   \n",
      "https://keskustelu.suomi24.fi/t/17746121/elisa-...  Kyllä maksaa jos aikoo saada turvallisen ja lu...   \n",
      "https://keskustelu.suomi24.fi/t/17746121/elisa-...  Osakeyhtiön ainoa tarkoitus on tuottaa osakkai...   \n",
      "https://keskustelu.suomi24.fi/t/17772514/onko-e...  Elisan Palstalle kun laittaa viestiä niin vast...   \n",
      "\n",
      "                                                    replyCount  \\\n",
      "url                                                              \n",
      "https://keskustelu.suomi24.fi/t/17812132/miten-...          10   \n",
      "https://keskustelu.suomi24.fi/t/17746121/elisa-...           8   \n",
      "https://keskustelu.suomi24.fi/t/17746121/elisa-...           6   \n",
      "https://keskustelu.suomi24.fi/t/17746121/elisa-...           6   \n",
      "https://keskustelu.suomi24.fi/t/17746121/elisa-...           6   \n",
      "...                                                        ...   \n",
      "https://keskustelu.suomi24.fi/t/17746121/elisa-...           0   \n",
      "https://keskustelu.suomi24.fi/t/17816402/mitka-...           0   \n",
      "https://keskustelu.suomi24.fi/t/17746121/elisa-...           0   \n",
      "https://keskustelu.suomi24.fi/t/17746121/elisa-...           0   \n",
      "https://keskustelu.suomi24.fi/t/17772514/onko-e...           0   \n",
      "\n",
      "                                                                                               parent  \n",
      "url                                                                                                    \n",
      "https://keskustelu.suomi24.fi/t/17812132/miten-...  https://keskustelu.suomi24.fi/t/17812132/miten...  \n",
      "https://keskustelu.suomi24.fi/t/17746121/elisa-...  https://keskustelu.suomi24.fi/t/17746121/emeli...  \n",
      "https://keskustelu.suomi24.fi/t/17746121/elisa-...  https://keskustelu.suomi24.fi/t/17746121/emeli...  \n",
      "https://keskustelu.suomi24.fi/t/17746121/elisa-...  https://keskustelu.suomi24.fi/t/17746121/emeli...  \n",
      "https://keskustelu.suomi24.fi/t/17746121/elisa-...  https://keskustelu.suomi24.fi/t/17746121/emeli...  \n",
      "...                                                                                               ...  \n",
      "https://keskustelu.suomi24.fi/t/17746121/elisa-...  https://keskustelu.suomi24.fi/t/17746121/emeli...  \n",
      "https://keskustelu.suomi24.fi/t/17816402/mitka-...  https://keskustelu.suomi24.fi/t/17816402/vat-t...  \n",
      "https://keskustelu.suomi24.fi/t/17746121/elisa-...  https://keskustelu.suomi24.fi/t/17746121/emeli...  \n",
      "https://keskustelu.suomi24.fi/t/17746121/elisa-...  https://keskustelu.suomi24.fi/t/17746121/emeli...  \n",
      "https://keskustelu.suomi24.fi/t/17772514/onko-e...  https://keskustelu.suomi24.fi/t/17772514/onko-...  \n",
      "\n",
      "[80 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(allComments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0eb1c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
